{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models with Modular Data Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T12:02:29.237351Z",
     "start_time": "2024-09-08T12:02:29.096351Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys; sys.path.append(\"../../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T12:02:36.322591Z",
     "start_time": "2024-09-08T12:02:29.758349Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import sensai\n",
    "import sensai.xgboost\n",
    "import sensai.torch\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VectorModel\n",
    "\n",
    "The backbone of supervised learning implementations is the `VectorModel` abstraction.\n",
    "It is so named, because, in computer science, a *vector* corresponds to an array of data,\n",
    "and vector models map such vectors to the desired outputs, i.e. regression targets or \n",
    "classes.\n",
    "\n",
    "It is important to note that this does *not* limit vector models to tabular data, because the data within\n",
    "a vector can take arbitrary forms (in contrast to vectors as they are defined in mathematics).\n",
    "Every element of an input vector could itself be arbitrarily\n",
    "complex, and could, in the most general sense, be any kind of object.\n",
    "\n",
    "### The VectorModel Class Hierarchy\n",
    "\n",
    "`VectorModel` is an abstract base class.\n",
    "From it, abstract base classes for classification (`VectorClassificationModel`) and regression (`VectorRegressionModel`) are derived. And we furthermore provide base classes for rule-based models, facilitating the implementation of models that do not require learning (`RuleBasedVectorClassificationModel`, `RuleBasedVectorRegressionModel`).\n",
    "\n",
    "These base classes are, in turn, specialised in order to provide direct access to model implementations based on widely used machine learning libraries such as scikit-learn, XGBoost, PyTorch, etc.\n",
    "Use your IDE's hierarchy view to inspect them.\n",
    "\n",
    "<!-- TODO: hierarchical bullet item list with hierarchy (or maybe auto-generate?) -->\n",
    "\n",
    "### DataFrame-Based Interfaces\n",
    "\n",
    "Vector models use pandas DataFrames as the fundmental input and output data structures.\n",
    "Every row in a data frame corresponds to a vector of data, and an entire data frame can thus be viewed as a dataset or batch of data. Data frames are a good base representation for input data because\n",
    "  * they provide rudimentary meta-data in the form of column names, avoiding ambiguity.\n",
    "  * they can contain arbitrarily complex data, yet in the simplest of cases, they can directly be mapped to a data matrix (2D array) of features that simple models can directly process.\n",
    "\n",
    "The `fit` and `predict` methods of `VectorModel` take data frames as input, and the latter furthermore returns its predictions as a data frame.\n",
    "It is important to note that the DataFrame-based interface does not limit the scope of the models that can be applied, as one of the key principles of vector models is that they may define arbitrary model-specific transformations of the data originally contained in a data frame (e.g. a conversion from complex objects in data frames to one or more tensors for neural networks), as we shall see below.\n",
    "\n",
    "Here's the particularly simple Iris dataset for flower species classification, where the features are measurements of petals and sepals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sensai.data.dataset.DataSetClassificationIris()\n",
    "io_data = dataset.load_io_data()\n",
    "io_data.to_df().sample(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `io_data` is an instance of `InputOutputData`, which contains two data frames, `inputs` and `outputs`.\n",
    "The `to_df` method merges the two data frames into one for easier visualisation.\n",
    "\n",
    "Let's split the dataset and apply a model to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and split a dataset\n",
    "splitter = sensai.data.DataSplitterFractional(0.8)\n",
    "train_io_data, test_io_data = splitter.split(io_data)\n",
    "\n",
    "# train a model\n",
    "model = sensai.sklearn.classification.SkLearnRandomForestVectorClassificationModel(\n",
    "    n_estimators=15)\n",
    "model.fit_input_output_data(train_io_data)\n",
    "\n",
    "# make predictions\n",
    "predictions = model.predict(test_io_data.inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit_input_output_data` method is just a convenience method to pass an `InputOutputData` instance instead of two data frames. It is equivalent to\n",
    "\n",
    "```python\n",
    "model.fit(train_io_data.inputs, train_io_data.outputs)\n",
    "```\n",
    "\n",
    "where the two data frames containing inputs and outputs are passed separately.\n",
    "\n",
    "Now let's compare the ground truth to some of the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat((test_io_data.outputs, predictions), axis=1).sample(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Custom Models\n",
    "\n",
    "It is straightforward to implement your own model. Simply subclass the appropriate base class depending on the type of model you want to implement.\n",
    "\n",
    "For example, let us implement a simple classifier where we always return the a priori probability of each class in the training data, ignoring the input data for predictions. For this case, we inherit from `VectorClassificationModel` and implement the two abstract methods it defines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriorProbabilityVectorClassificationModel(sensai.VectorClassificationModel):\n",
    "    def _fit_classifier(self, x: pd.DataFrame, y: pd.DataFrame):\n",
    "        self._prior_probabilities = y.iloc[:, 0].value_counts(normalize=True).to_dict()\n",
    "\n",
    "    def _predict_class_probabilities(self, x: pd.DataFrame) -> pd.DataFrame:\n",
    "        values = [self._prior_probabilities[cls] for cls in self.get_class_labels()]\n",
    "        return pd.DataFrame([values] * len(x), columns=self.get_class_labels(), index=x.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapting a model implementation from another machine learning library is typically just a few lines. For models that adhere to the scikit-learn interfaces for learning and prediction, there are abstract base classes that make the adaptation particularly straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "Apart from the parameters passed at construction, which are specific to the type of model in question, all vector models can be flexibly configured via methods that can be called post-construction.\n",
    "These methods all have the `with_` prefix, indicating that they return the instance itself (akin to the builder pattern), allowing calls to be chained in a single statement.\n",
    "\n",
    "The most relevant such methods are:\n",
    "\n",
    "* `with_name` to name the model (for reporting purposes)\n",
    "* `with_raw_input_transformer` for adding an initial input transformation\n",
    "* `with_feature_generator` and `with_feature_collector` for specifying how to generate features from the input data\n",
    "* `with_feature_transformers` for specifying how the generated features shall be transformed\n",
    "\n",
    "The latter three points are essential for defining modular input pipelines and will be addressed in detail below.\n",
    "\n",
    "All configured options are fully reflected in the model's string representation, which can be pretty-printed with the `pprint` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(model.with_name(\"RandomForest\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modular Pipelines\n",
    "\n",
    "A key principle of sensAI's vector models is that data pipelines \n",
    "* can be **strongly associated with a model**. This is critically important if several heterogeneous models shall be applied to the same use case. Typically, every model has different requirements regarding the data it can process and the representation it requires to process it optimally.\n",
    "* are to be **modular**, meaning that a pipeline can be composed from reusable and user-definable components.\n",
    "\n",
    "An input pipeline typically serves the purpose of answering the following questions:\n",
    "\n",
    "* **How shall the data be pre-processed?**\n",
    "\n",
    "  It might be necessary to process the data before we can use it and extract meaningful features from it.\n",
    "  We may need to filter or clean the data;\n",
    "  we may need to establish a usable representation from raw data (e.g. convert a string-based representation of a date into a proper data structure);\n",
    "  or we may need to infer/impute missing data.\n",
    "\n",
    "  The relevant abstraction for this task is `DataFrameTransformer`, which, as the name suggests, can arbitrarily transform a data frame.\n",
    "  All non-abstract class implementations have the prefix `DFT` in sensAI and thus are easily discovered through auto-completion.\n",
    "\n",
    "  A `VectorModel` can be configured to apply a pre-processing transformation via method `with_raw_input_transformers`.\n",
    "\n",
    "* **What is the data used by the model?**\n",
    "\n",
    "  The relevant abstraction is `FeatureGenerator`. Via `FeatureGenerator` instances, a model can define which set of features is to be used. Moreover, these instances can hold meta-data on the respective features, which can be leveraged for downstream representation. \n",
    "  In sensAI, the class names of all feature generator implementations use the prefix `FeatureGenerator`.\n",
    "\n",
    "  A `VectorModel` can be configured to answer this question via method `with_feature_generator` (or `with_feature_collector`).\n",
    "\n",
    "* **How does that data need to be represented?**\n",
    "\n",
    "  Different models can require different representations of the same data. For example, some models might require all features to be numeric, thus requiring categorical features to be encoded, while others might work better with the original representation.\n",
    "  Furthermore, some models might work better with numerical features normalised or scaled in a certain way while it makes no difference to others.\n",
    "  We can address these requirements by adding model-specific transformations.\n",
    "\n",
    "  The relevant abstraction is, once again, `DataFrameTransformer`.\n",
    "\n",
    "  A `VectorModel` can be configured to apply a transformation to its features via method `with_feature_transformers`.\n",
    "\n",
    "The three pipeline stages are applied in the order presented above, and all components are optional, i.e. if a model does not define any raw input transformers, then the original data remains unmodified. If a model defines no feature generator, then the set of features is given by the full input data frame, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data Frame Transformers (DFTs)\n",
    "\n",
    "As the name suggests, a data frame transformer (DFT) is a simple concept: It simply transforms a data frame into a new data frame. The transformation can either be pre-defined or be learnt from data. A common case is for the new data frame to contain a modified representation of the same data.\n",
    "\n",
    "The package `sensai.data_transformation` contains a multitude of concrete transformers that can directly be applied as well as base classes for custom transformer implementations.\n",
    "\n",
    "As an example, consider a data frame containing a column with string representations of points in time:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T12:10:35.534279Z",
     "start_time": "2024-09-08T12:10:35.198754Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = {'ts': ['2024-09-08 12:31:18', '2022-09-10 18:31:12', None, '2022-09-12 07:55:05']}\n",
    "raw_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can define a data frame transformer that will convert the string representations into proper Timestamp objects for downstream processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T12:09:07.259528Z",
     "start_time": "2024-09-08T12:09:07.075530Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DFTStringToTimestamp(sensai.data_transformation.RuleBasedDataFrameTransformer):\n",
    "    def _apply(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        df[\"ts\"] = pd.to_datetime(df[\"ts\"])\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Because this transformation does not require any learning, we have derived it from `RuleBasedDataFrameTransformer` and the only method to be implemented is the private method that applies the transformation.\n",
    "\n",
    "Applying the transformation is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T12:09:08.939673Z",
     "start_time": "2024-09-08T12:09:08.731673Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dft_string_to_timestamp = DFTStringToTimestamp()\n",
    "transformed_df = dft_string_to_timestamp.apply(raw_df)\n",
    "transformed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "DFTs can be chained via `DataFrameTranformerChain`, which will apply transformations sequentially.\n",
    "A `DataFrameTransformerChain` is itself a `DataFrameTransformer`, allowing for the definition of complex pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T12:23:32.534291Z",
     "start_time": "2024-09-08T12:23:30.935978Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dft_chain = sensai.data_transformation.DataFrameTransformerChain(\n",
    "    DFTStringToTimestamp(), sensai.data_transformation.DFTDropNA())\n",
    "transformed_df = dft_chain.apply(raw_df)\n",
    "transformed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `DataFrameTransformerChain` can also be created by using the `chain` method of `DataFrameTransformer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DFTStringToTimestamp().chain(sensai.data_transformation.DFTDropNA()).apply(raw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Generators\n",
    "\n",
    "Feature generators serve two main functions:\n",
    "\n",
    "1. They define how features can be generated from the input data frame.\n",
    "\n",
    "2. They hold meta-data on the generated features, which can be leveraged for downstream transformation. Specifically,\n",
    "   * we can define which features are categorical,\n",
    "   * we can define rules for normalisation or scaling of numerical features. \n",
    "\n",
    "The basic functionality of a feature generator is to create, from an input data frame, a data frame with the same index that contains one or more columns, each column representing a feature that the model shall use.\n",
    "\n",
    "#### A Simple, Rule-Based Feature Generator\n",
    "\n",
    "Let's consider a simple example: Suppose we have the transformed data frame with timestamps from above and want the model to use the hour of the day as a feature. Since this feature generator does not require learning, we can define a `RuleBasedFeatureGenerator` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureGeneratorHourOfDay(sensai.featuregen.RuleBasedFeatureGenerator):\n",
    "    def _generate(self, df: pd.DataFrame, ctx=None) -> pd.DataFrame:\n",
    "        hour_series = df[\"ts\"].apply(lambda t: t.hour)\n",
    "        return pd.DataFrame({\"hour_of_day\": hour_series}, index=df.index)\n",
    "    \n",
    "FeatureGeneratorHourOfDay().generate(transformed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a most simple example, where the feature generation mechanism must not be learned from data.\n",
    "If we require the generator to adapt itself to the training data, we can instead derive our class from `FeatureGenerator` and implement method `_fit` accordingly.\n",
    "\n",
    "#### Making Use of Base Classes\n",
    "\n",
    "sensAI provides a wide variety of base classes that simplify the definition of feature generators, including\n",
    "\n",
    " * :py:class:`sensai.featuregen.feature_generator.FeatureGeneratorTakeColumns`, which simply takes over columns from the input data frame without modifying them\n",
    " * :py:class:`sensai.featuregen.feature_generator.FeatureGeneratorMapColumn`, which maps the values of an input column to a new feature column\n",
    " * :py:class:`sensai.featuregen.feature_generator.FeatureGeneratorFlattenColumns`, which generates features by flattening one or more vector-valued columns in the input\n",
    " * :py:class:`sensai.featuregen.feature_generator.FeatureGeneratorMapColumnDict`, which maps an input column to several feature columns, i.e. mapping each input value to a dictionary of output values\n",
    " * :py:class:`sensai.featuregen.feature_generator.FeatureGeneratorFromVectorModel`, which generates features by applying a (regression or classifcation) model to the input data frame\n",
    " * :py:class:`sensai.featuregen.feature_generator.FeatureGeneratorFromDFT`, which generates features by applying a given data frame transformer to the input\n",
    " * :py:class:`sensai.featuregen.feature_generator.FeatureGeneratorFromColumnGenerator`, which uses the concept of a `ColumnGenerator` to implement a feature generator, which specifically supports index-based caching mechanisms for feature generation\n",
    " * :py:class:`sensai.featuregen.feature_generator.FeatureGeneratorTargetDistribution`, which computes conditional distributions of the (optionally discretised) target variable given one or more categorical features in the input data\n",
    "\n",
    "As a simple example, let us use `FeatureGeneratorMapColumn` to implement a second feature based on the timestamp column from the earlier data frame: the day of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureGeneratorDayOfWeek(sensai.featuregen.feature_generator.FeatureGeneratorMapColumn):\n",
    "    def __init__(self):\n",
    "        super().__init__(input_col_name=\"ts\", feature_col_name=\"day_of_week\")\n",
    "\n",
    "    def _create_value(self, ts: pd.Timestamp):\n",
    "        return ts.day_of_week\n",
    "    \n",
    "FeatureGeneratorDayOfWeek().generate(transformed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Meta-Data for Downstream Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to apply the \"hour of day\" feature generator within a neural network, where we might want to normalise the features.\n",
    "We can do this by extending the implementation with a constructor that defines a normalisation rule template, which is to apply to the generated column.\n",
    "\n",
    "In general, we can define specific normalisation rules for every feature generated by a feature generator, but if the same rule shall apply to all columns (or if there is only one), the use of a normalisation rule template avoids the unnecessary repetition of column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sensai.data_transformation.dft import DFTNormalisation\n",
    "from sensai.data_transformation.sklearn_transformer import SkLearnTransformerFactoryFactory\n",
    "\n",
    "\n",
    "class FeatureGeneratorHourOfDay(sensai.featuregen.RuleBasedFeatureGenerator):\n",
    "    def __init__(self):\n",
    "        super().__init__(normalisation_rule_template=DFTNormalisation.RuleTemplate(\n",
    "            transformer_factory=SkLearnTransformerFactoryFactory.ManualScaler(scale=1/23)))\n",
    "\n",
    "    def _generate(self, df: pd.DataFrame, ctx=None) -> pd.DataFrame:\n",
    "        hour_series = df[\"ts\"].apply(lambda t: t.hour)\n",
    "        return pd.DataFrame({\"hour_of_day\": hour_series}, index=df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already explained above, the normalisation rules and categorical feature data are only meta-data; the normalisation rule template we specified does not affect the actual generation of the feature. Why? Because we want to be able to use the same feature generator with different types of models - and the models shall decide whether they want to use that information in order to apply transformations they need to operate optimally.\n",
    "\n",
    "In the present case, the normalisation rule provides information for the data frame transformer class `DFTNormalisation`, and here's how it could be applied manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_hour = FeatureGeneratorHourOfDay()\n",
    "DFTNormalisation(rules=fg_hour.get_normalisation_rules()).fit_apply(fg_hour.generate(transformed_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we might want to use one-hot encoding for the \"day of the week\" feature, as this is essentially categorical information; higher integer values do not indicate \"more of something\" but are entirely different categories. We can similarly extend the earlier definition of the feature generator and declare that the feature is categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sensai.data_transformation.dft import DFTOneHotEncoder\n",
    "\n",
    "\n",
    "class FeatureGeneratorDayOfWeek(sensai.featuregen.feature_generator.FeatureGeneratorMapColumn):\n",
    "    def __init__(self):\n",
    "        super().__init__(input_col_name=\"ts\", feature_col_name=\"day_of_week\", categorical_feature_names=[\"day_of_week\"])\n",
    "\n",
    "    def _create_value(self, ts: pd.Timestamp):\n",
    "        return ts.day_of_week\n",
    "    \n",
    "\n",
    "fg_day = FeatureGeneratorDayOfWeek()\n",
    "DFTOneHotEncoder(fg_day.get_categorical_feature_name_regex()).fit_apply(fg_day.generate(transformed_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying these downstream transformations for normalisation and one-hot encoding is, of course, cumbersome, which is why sensAI offers a convenient concept to simplify such applications: feature collectors.\n",
    "\n",
    "### Feature Collectors\n",
    "\n",
    "Feature collectors facilitate the combination of several feature generators into a single `MultiFeatureGenerator` that generates the full feature data frame.\n",
    "They furthermore enable the convenient creation of downstream feature transformers for scaling/normalisation and the encoding of categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_collector = sensai.featuregen.FeatureCollector(\n",
    "    FeatureGeneratorHourOfDay(), FeatureGeneratorDayOfWeek())\n",
    "\n",
    "multi_feature_gen = feature_collector.create_multi_feature_generator()\n",
    "features_df = multi_feature_gen.generate(transformed_df)\n",
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the downstream transformers using the feature collector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft_one_hot_encoder = feature_collector.create_dft_one_hot_encoder()\n",
    "dft_normalisation = feature_collector.create_dft_normalisation()\n",
    "\n",
    "feature_transformer = dft_one_hot_encoder.chain(dft_normalisation)\n",
    "feature_transformer.fit_apply(features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Defining a Vector Model's Input Pipeline\n",
    "\n",
    "Now let's put it all together and define the full input pipeline of a model based on the above definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_collector = sensai.featuregen.FeatureCollector(\n",
    "    FeatureGeneratorHourOfDay(), FeatureGeneratorDayOfWeek())\n",
    "\n",
    "mlp_model = sensai.sklearn.sklearn_classification.SkLearnMLPVectorClassificationModel() \\\n",
    "    .with_raw_input_transformers(DFTStringToTimestamp(), sensai.data_transformation.DFTDropNA()) \\\n",
    "    .with_feature_collector(feature_collector) \\\n",
    "    .with_feature_transformers(feature_collector.create_dft_one_hot_encoder(), feature_collector.create_dft_normalisation())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This declaration makes the model performs the full set of transformations that we considered earlier.\n",
    "Recall the original input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit the model's preprocessors (i.e. its input pipeline), using some dummy classification targets `y` for the `fit` call to be applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame({\"target\": [True, True, True, False]})\n",
    "mlp_model.fit(raw_df, y, fit_preprocessors=True, fit_model=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every vector model supports the method `compute_model_inputs` to run the input pipeline and generate the data that would actually be passed to the underlying model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model.compute_model_inputs(raw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Registries\n",
    "\n",
    "When experimenting with different models, features and their representation play a central role.\n",
    "As we add new features or different representations of the information we have, we require a concise and explicit way of defining the set of features a model shall use. \n",
    "The use of a registry enables this: Using `FeatureGeneratorRegistry`, we can refer to features by names or other hashable types.\n",
    "\n",
    "Especially in larger projects, the use of an enum comprising the set of features is recommended. Let us define a registry containing the two features we considered above: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class FeatureName(Enum):\n",
    "    HOUR_OF_DAY = \"hour_of_day\"\n",
    "    DAY_OF_WEEK = \"day_of_week\"\n",
    "\n",
    "registry = sensai.featuregen.FeatureGeneratorRegistry()\n",
    "registry.register_factory(FeatureName.HOUR_OF_DAY, lambda: FeatureGeneratorHourOfDay())\n",
    "registry.register_factory(FeatureName.DAY_OF_WEEK, lambda: FeatureGeneratorDayOfWeek())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this registry, we can obtain a feature collector for use in a model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_collector = registry.collect_features(FeatureName.HOUR_OF_DAY, FeatureName.DAY_OF_WEEK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Use Case: Titanic Survival\n",
    "\n",
    "#### The Titanic Survival Data Set\n",
    "\n",
    "Let us consider the well-known Titanic Survival data set as an example.\n",
    "\n",
    "Every data point holds data on a passenger. The data set has the following potentially predictive columns,\n",
    "\n",
    "* `Pclass`: the passenger ticket class as an integer (1=first, 2=second, 3=third)\n",
    "* `Sex`: the passenger's sex (male or female)\n",
    "* `Age`: the passenger's age in years (integer); this feature is frequently missing\n",
    "* `SibSp`: the number of siblings and spouses of the passenger\n",
    "* `Parch`: the number of parents and children of the passenger\n",
    "* `Fare`: the fare price paid\n",
    "* `Embark`: the port of embarkation (C=Cherbourg, Q=Queenstown, S=Southampton); this feature is missing for two passengers\n",
    "\n",
    "and some further meta-data columns (Name, Cabin).\n",
    "\n",
    "The goal is to predict the column 'Survived' indicating whether the passenger survived (1) or not (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sensai.data.dataset.DataSetClassificationTitanicSurvival()\n",
    "io_data = dataset.load_io_data()\n",
    "io_data.to_df().iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw Input Transformers (Pre-Processing)\n",
    "\n",
    "We shall now add pipeline components to an XGBoost model, as it can straightforwardly deal with missing data.\n",
    "\n",
    "The dataset doesn't really require any pre-processing, but we could (just for demonstration purposes)\n",
    "* get rid of the useless meta-data columns,\n",
    "* convert the passenger class feature into a string to ensure that it is not treated as a numerical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFTTitanicDropMetaDataColumns(sensai.data_transformation.DFTColumnFilter):\n",
    "    def __init__(self):\n",
    "        super().__init__(drop=[dataset.COL_NAME, dataset.COL_CABIN, dataset.COL_TICKET])\n",
    "        \n",
    "class DFTTitanicTransformPassengerClass(sensai.data_transformation.DFTModifyColumn):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            column=dataset.COL_PASSENGER_CLASS, \n",
    "            column_transform=lambda n: {1: \"first\", 2: \"second\", 3: \"third\"}[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try applying them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DFTTitanicDropMetaDataColumns().chain(DFTTitanicTransformPassengerClass()).apply(io_data.inputs).iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Generators for Titanic Survival\n",
    "\n",
    "In the Titanic Survival data set, the features are already fully prepared, so we do not need to actually generate anything; \n",
    "we can simply take the feature values as they are present in the original data frame and add only the necessary meta-data.\n",
    "As mentioned above, the base class for this purpose is `FeatureGeneratorTakeColumns`, which allows us to take over columns directly from the input data.\n",
    "We could use a single feature generator for all features as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FeatureGeneratorTitanicAll(sensai.featuregen.FeatureGeneratorTakeColumns):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            columns=None,  # take all columns\n",
    "            categorical_feature_names=[dataset.COL_SEX, dataset.COL_PASSENGER_CLASS, dataset.COL_PORT_EMBARKED],\n",
    "            normalisation_rule_template=sensai.featuregen.DFTNormalisation.RuleTemplate(\n",
    "                transformer_factory=sensai.data_transformation.SkLearnTransformerFactoryFactory.MaxAbsScaler(),\n",
    "                independent_columns=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We have supplied both meta-data regarding \n",
    "* the subset of feature that are categorical\n",
    "* the normalisation rule to be applied to the numerical features (if normalisation is applied with `DFTNormalisation`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Models with Customised Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let us now define two models, an XGBoost model as well as a torch-based multi-layer perceptron (MLP) model based on the raw input transformers and the feature generator defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_collector = sensai.featuregen.FeatureCollector(FeatureGeneratorTitanicAll())\n",
    "\n",
    "xgb_model = sensai.xgboost.XGBGradientBoostedVectorClassificationModel() \\\n",
    "    .with_raw_input_transformers(\n",
    "        DFTTitanicDropMetaDataColumns(),\n",
    "        DFTTitanicTransformPassengerClass()) \\\n",
    "    .with_name(\"XGBoost\") \\\n",
    "    .with_feature_collector(feature_collector, shared=True) \\\n",
    "    .with_feature_transformers(\n",
    "        feature_collector.create_feature_transformer_one_hot_encoder(ignore_unknown=True))\n",
    "\n",
    "torch_mlp_model = sensai.torch.models.MultiLayerPerceptronVectorClassificationModel(\n",
    "        hid_activation_function=torch.relu,\n",
    "        hidden_dims=[10, 10, 4],\n",
    "        cuda=False,\n",
    "        p_dropout=0.25,\n",
    "        nn_optimiser_params=sensai.torch.NNOptimiserParams(early_stopping_epochs=10)) \\\n",
    "    .with_name(\"MLP\") \\\n",
    "    .with_raw_input_transformers(\n",
    "        DFTTitanicDropMetaDataColumns(),\n",
    "        DFTTitanicTransformPassengerClass()) \\\n",
    "    .with_feature_collector(feature_collector, shared=True) \\\n",
    "    .with_feature_transformers(\n",
    "        sensai.data_transformation.DFTColumnFilter(drop=[dataset.COL_PORT_EMBARKED, dataset.COL_AGE_YEARS]),\n",
    "        feature_collector.create_feature_transformer_one_hot_encoder(ignore_unknown=True),\n",
    "        feature_collector.create_dft_normalisation())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Notice that the model definitions are purely declarative: We define each model and the respective feature pipeline by injecting appropriate pipeline components.\n",
    "\n",
    "Both models use one-hot encoding of categorical features.\n",
    "For the multi-layer perceptron model, we notably added some additional feature transformers:\n",
    "* Since this type model cannot cope with missing feature values, we added a component that drops the age and port columns, which are sometimes undefined.\n",
    "* Since neural networks work best with normalised feature representations, we added the normalisation component, which uses a standard scaler (as defined in the feature generator).\n",
    "\n",
    "\n",
    "#### Evaluating Models\n",
    "\n",
    "We define an evaluation object for the data set and subsequently apply it in order to compare the two models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_params = sensai.evaluation.ClassificationEvaluatorParams(fractional_split_test_fraction=0.2)\n",
    "titanic_evaluation = sensai.evaluation.ClassificationModelEvaluation(io_data, evaluator_params=evaluator_params)\n",
    "\n",
    "titanic_evaluation.compare_models([xgb_model, torch_mlp_model]).results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Complex Use Cases\n",
    "\n",
    "For some more complex example applications, see the [examples folder in the sensAI repository](https://github.com/opcode81/sensAI/tree/develop/examples/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
